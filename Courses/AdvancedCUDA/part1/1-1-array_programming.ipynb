{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/github/Learning/Courses/AdvancedCUDA/part1`\n",
      "┌ Warning: The active manifest file has dependencies that were resolved with a different julia version (1.10.5). Unexpected behavior may occur.\n",
      "└ @ nothing /home/araki/github/Learning/Courses/AdvancedCUDA/part1/Manifest.toml:0\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()\n",
    "\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Array programming\n",
    "\n",
    "In this notebook, I'll explain how to use the `CuArray` type to program the GPU. This is a convenient programming model that does not require detailed knowledge of the GPU, but there's still some noteworthy tips and tricks that can significantly impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all starts with the `CuArray` type, which serves a dual purpose:\n",
    "\n",
    "- a managed container for GPU memory\n",
    "- a way to dispatch to operations that execute on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 1.0  2.0\n",
       " 3.0  4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = CuArray([1. 2.; 3. 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common shorthand way to create a `CuArray` is to call the `cu` function. It behaves like an recursive, but opiniated constructor:\n",
    "- it descends into structures, e.g., `cu(Adjoint([1, 2])) == Adjoint(CuArray([1, 2]))`\n",
    "- it convers slow `Float64` into much faster `Float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 adjoint(::CuArray{Float32, 2, CUDA.DeviceMemory}) with eltype Float32:\n",
       " 1.0  3.0\n",
       " 2.0  4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cu([1. 2.; 3. 4.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 1.0  3.0\n",
       " 2.0  4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare to\n",
    "CuArray([1. 2.; 3. 4.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 1.0  3.0\n",
       " 2.0  4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare to\n",
    "CuArray([1.0 2.0; 3.0 4.0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory management will be discussed in detail in a later notebook, but for now it's enough to remember that a CuArray is **a CPU object representing memory on the GPU**. It will be automatically freed when all references have been removed, and the garbage collector runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of `CuArray` is to make it easy to program GPUs using array operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       "  7.0  10.0\n",
       " 15.0  22.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this will automatically use CUBLAS\n",
    "A * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 1.0   4.0\n",
       " 9.0  16.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# whereas this operation will use a native broadcast kernel\n",
    "A .* A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works by specializing certain methods with a GPU-specialized implementation, either for:\n",
    "- compatibility: not all CPU implementations work on the GPU\n",
    "- performance: GPUs have a different programming model so might require optimized implementations\n",
    "\n",
    "This generally works pretty well, the goal is to get as close to the CPU `Array` type's functionality as possible, and entire applications have been built on top of CuArray's array functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-order functionality\n",
    "\n",
    "The broadcast expression `A .* A` may look like a simple, special-purpose element-wise multiplication, but is syntactical sugar for a much more generic operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 1.0   4.0\n",
       " 9.0  16.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elwise_op(a, b) = a * b\n",
    "broadcast(elwise_op, A, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a higher-order operation, i.e., an operation that takes a function as an argument. This is a very powerful concept, because it makes it possible to *compose* the library definition of an operation, here `broadcast`, with user-provided code. This is possible in Julia because we have a JIT compiler, and in many cases **makes it possible to write custom GPU code without ever having to write a kernel**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing user code to a function in Julia can also be done using the `do-block` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 1.0   4.0\n",
       " 9.0  16.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "broadcast(A, A) do a, b\n",
    "    a * b\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the most convenient syntax is of course the dot syntax, where e.g. `f.(A .+ B)` is equivalent to broadcasting a function that computes `f(a + b)` for each element `a` in `A` and `b` in `B`. It's important to note that this dot expression performing multiple operations resulted in only a single broadcast invocation, i.e., we have **syntactical dot fusion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":($(Expr(:thunk, CodeInfo(\n",
       "   \u001b[33m @ none within `top-level scope`\u001b[39m\n",
       "\u001b[90m1 ─\u001b[39m %1 = -\n",
       "\u001b[90m│  \u001b[39m %2 = f\n",
       "\u001b[90m│  \u001b[39m %3 = B\n",
       "\u001b[90m│  \u001b[39m %4 = Base.broadcasted(+, A, %3)\n",
       "\u001b[90m│  \u001b[39m %5 = Base.broadcasted(%2, %4)\n",
       "\u001b[90m│  \u001b[39m %6 = Base.broadcasted(%1, %5, 2)\n",
       "\u001b[90m│  \u001b[39m %7 = Base.materialize(%6)\n",
       "\u001b[90m└──\u001b[39m      return %7\n",
       "))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Meta.@lower f.(A .+ B) .- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia defines a variety of these higher-order operations, many of which are implemented by the GPU back-ends. For example, there's also `map`, similar to `broadcast` but without the, well, broadcasting property that allows for mismatching sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 2.0  4.0\n",
       " 6.0  8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map(A) do a\n",
    "    a * 2\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions\n",
    "\n",
    "Another important higher-order operation is `mapreduce`, which can be used to map & reduce any part of an N-dimensional array. For example, one of the simplest invocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = cu([1, 2])\n",
    "reduce(+, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces to a scalar, which requires synchronizing the GPU. Instead, you can also synchronize to a one-element array, which will then only synchronize the GPU when fetching the contents of that array. This is done by specifying the `dims` keyword, which specifies which dimensions to reduce over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduce(+, A; dims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dims` keyword also makes it possible to perform multiple reductions at once, e.g., in the case the latest dimension of an array represents the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1×3 CuArray{Float32, 3, CUDA.DeviceMemory}:\n",
       "[:, :, 1] =\n",
       " 51.145782\n",
       "\n",
       "[:, :, 2] =\n",
       " 50.320923\n",
       "\n",
       "[:, :, 3] =\n",
       " 47.491447"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = CUDA.rand(10, 10, 3)\n",
    "reduce(+, A; dims=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduce` operation is part of a family of operations that all build on the `mapreduce` operation, with specializations like `sum`, `prod`, `any`, `all`, etc.\n",
    "\n",
    "A version of `reduce` that maintains the intermediate results is `accumulate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = cu(ones(5))\n",
    "accumulate(+, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module-level functionality\n",
    "\n",
    "Some Julia APIs do not take an array argument, and as such cannot be specialized for GPU execution. Examples include: `rand`, `fill`, `zeros`, etc. For these functions, CUDA provides unexported replacements, e.g., `CUDA.rand`, `CUDA.fill`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 0.560548  0.522902\n",
       " 0.94456   0.702343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.rand(Float64, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common issues\n",
    "\n",
    "Before trying this out, let's take a look at some issues that are common when using arrays to program the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar iteration\n",
    "\n",
    "A key performance issue comes from the fact that a `CuArray` instance is a CPU object representing a chunk of memory on the GPU. That means we invoke the GPU for every CPU operation invoked on a CuArray. That is OK for array operations, where the GPU will have to do a bunch of work, but is very bad when you have CPU code performing a bunch of small scalar operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore should be avoided.\n\nIf you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\nto enable scalar iteration globally or for the operations in question.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore should be avoided.\n",
      "\n",
      "If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n",
      "to enable scalar iteration globally or for the operations in question.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] errorscalar(op::String)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:155\n",
      " [3] _assertscalar(op::String, behavior::GPUArraysCore.ScalarIndexing)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:128\n",
      " [4] assertscalar(op::String)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:116\n",
      " [5] getindex(A::CuArray{Int64, 1, CUDA.DeviceMemory}, I::Int64)\n",
      "   @ GPUArrays ~/.julia/packages/GPUArrays/qt4ax/src/host/indexing.jl:50\n",
      " [6] top-level scope\n",
      "   @ ~/github/Learning/Courses/AdvancedCUDA/part1/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X46sdnNjb2RlLXJlbW90ZQ==.jl:4"
     ]
    }
   ],
   "source": [
    "A = CuArray(1:10)\n",
    "A_sum = zero(eltype(A))\n",
    "for I in eachindex(A)\n",
    "    A_sum += A[I]\n",
    "end\n",
    "A_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this kind of programming pattern, iterating the array and fetching one scalar at a time (hence 'scalar iteration'), being so slow CUDA.jl warns about it. With the above snippet, the situation is actually even worse: Not only does every iteration require a GPU operation to fetch an element, the `getindex` call is also the only array operation meaning that the actual summation won't even run on the GPU!\n",
    "\n",
    "The solution here is to use the `sum` function that performs the entire operation as a single step.\n",
    "\n",
    "To disallow scalar iteration, use the `allowscalar` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore should be avoided.\n\nIf you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\nto enable scalar iteration globally or for the operations in question.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore should be avoided.\n",
      "\n",
      "If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n",
      "to enable scalar iteration globally or for the operations in question.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] errorscalar(op::String)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:155\n",
      " [3] _assertscalar(op::String, behavior::GPUArraysCore.ScalarIndexing)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:128\n",
      " [4] assertscalar(op::String)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:116\n",
      " [5] getindex(A::CuArray{Int64, 1, CUDA.DeviceMemory}, I::Int64)\n",
      "   @ GPUArrays ~/.julia/packages/GPUArrays/qt4ax/src/host/indexing.jl:50\n",
      " [6] top-level scope\n",
      "   @ ~/github/Learning/Courses/AdvancedCUDA/part1/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X51sdnNjb2RlLXJlbW90ZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "A[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should generally always enable this option! It's not by default in interactive sessions because it simplifies porting CPU code, and it's easy to trigger scalar iteration from non performance-sensitive paths (e.g. display methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 adjoint(::CuArray{Int64, 1, CUDA.DeviceMemory}) with eltype Int64:\n",
       " 1  2  3  4  5  6  7  8  9  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore should be avoided.\n\nIf you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\nto enable scalar iteration globally or for the operations in question.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore should be avoided.\n",
      "\n",
      "If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n",
      "to enable scalar iteration globally or for the operations in question.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base ./error.jl:35\n",
      "  [2] errorscalar(op::String)\n",
      "    @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:155\n",
      "  [3] _assertscalar(op::String, behavior::GPUArraysCore.ScalarIndexing)\n",
      "    @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:128\n",
      "  [4] assertscalar(op::String)\n",
      "    @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:116\n",
      "  [5] getindex\n",
      "    @ ~/.julia/packages/GPUArrays/qt4ax/src/host/indexing.jl:50 [inlined]\n",
      "  [6] isassigned\n",
      "    @ ./multidimensional.jl:1612 [inlined]\n",
      "  [7] isassigned\n",
      "    @ ~/.julia/juliaup/julia-1.11.3+0.x64.linux.gnu/share/julia/stdlib/v1.11/LinearAlgebra/src/adjtrans.jl:331 [inlined]\n",
      "  [8] isassigned\n",
      "    @ ./multidimensional.jl:1607 [inlined]\n",
      "  [9] isassigned(::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true}, ::Int64, ::Int64)\n",
      "    @ Base ./subarray.jl:400\n",
      " [10] alignment(io::IOContext{IOBuffer}, X::AbstractVecOrMat, rows::Vector{Int64}, cols::Vector{Int64}, cols_if_complete::Int64, cols_otherwise::Int64, sep::Int64, ncols::Int64)\n",
      "    @ Base ./arrayshow.jl:68\n",
      " [11] _print_matrix(io::IOContext{IOBuffer}, X::AbstractVecOrMat, pre::String, sep::String, post::String, hdots::String, vdots::String, ddots::String, hmod::Int64, vmod::Int64, rowsA::UnitRange{Int64}, colsA::UnitRange{Int64})\n",
      "    @ Base ./arrayshow.jl:207\n",
      " [12] print_matrix(io::IOContext{IOBuffer}, X::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true}, pre::String, sep::String, post::String, hdots::String, vdots::String, ddots::String, hmod::Int64, vmod::Int64)\n",
      "    @ Base ./arrayshow.jl:171\n",
      " [13] print_matrix\n",
      "    @ ./arrayshow.jl:171 [inlined]\n",
      " [14] print_array\n",
      "    @ ./arrayshow.jl:358 [inlined]\n",
      " [15] show(io::IOContext{IOBuffer}, ::MIME{Symbol(\"text/plain\")}, X::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})\n",
      "    @ Base ./arrayshow.jl:399\n",
      " [16] limitstringmime(mime::MIME{Symbol(\"text/plain\")}, x::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})\n",
      "    @ VSCodeServer.IJuliaCore ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/IJuliaCore/src/inline.jl:22\n",
      " [17] display_mimestring(m::MIME{Symbol(\"text/plain\")}, x::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})\n",
      "    @ VSCodeServer.IJuliaCore ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/IJuliaCore/src/display.jl:67\n",
      " [18] display_dict(x::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})\n",
      "    @ VSCodeServer.IJuliaCore ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/IJuliaCore/src/display.jl:98\n",
      " [19] display(::VSCodeServer.JuliaNotebookInlineDisplay, x::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.DeviceMemory}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})\n",
      "    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/VSCodeServer/src/notebookdisplay.jl:32\n",
      " [20] display(x::Any)\n",
      "    @ Base.Multimedia ./multimedia.jl:340\n",
      " [21] #invokelatest#2\n",
      "    @ ./essentials.jl:1055 [inlined]\n",
      " [22] invokelatest\n",
      "    @ ./essentials.jl:1052 [inlined]\n",
      " [23] (::VSCodeServer.var\"#217#218\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:54\n",
      " [24] withpath(f::VSCodeServer.var\"#217#218\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/VSCodeServer/src/repl.jl:276\n",
      " [25] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:13\n",
      " [26] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [27] serve_notebook(pipename::String, debugger_pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; error_handler::var\"#5#10\"{String})\n",
      "    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:147\n",
      " [28] top-level scope\n",
      "    @ ~/.vscode-server/extensions/julialang.language-julia-1.127.2/scripts/notebook/notebook.jl:35"
     ]
    }
   ],
   "source": [
    "view(A', :, :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how Julia's type system works, it's easy to trigger non GPU-specialized methods when using array wrappers. Still, for non-interactive code it's recommended to always disable scalar iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, however, scalar iteration is perfectly fine. For example, when fetching the result of a reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510.49887f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = CUDA.rand(1024)\n",
    "R = sum(A; dims=1)\n",
    "CUDA.@allowscalar R[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be useful when the result of a reduction isn't immediately used, because reducing to an array can be executed asynchronously, while reducing to a scalar cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to avoid the issue of scalar iteration altogether by using unified memory, but more on that in a later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling into C libraries\n",
    "\n",
    "Another common issue arises when calling CPU-specific code, e.g. in some C library, using a GPU array. This generally does not work, because GPU pointers are not dereferencable on the CPU. To prevent this from crashing, we introduce a GPU-specific pointer type and disallow conversions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Illegal conversion of a CUDA.DeviceMemory to a Ptr{Float32}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Illegal conversion of a CUDA.DeviceMemory to a Ptr{Float32}\n",
      "\n",
      "Stacktrace:\n",
      " [1] convert(T::Type{Ptr{Float32}}, mem::CUDA.DeviceMemory)\n",
      "   @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/memory.jl:16\n",
      " [2] convert(::Type{Ptr{Float32}}, managed::CUDA.Managed{CUDA.DeviceMemory})\n",
      "   @ CUDA ~/.julia/packages/CUDA/2kjXI/src/memory.jl:577\n",
      " [3] unsafe_convert(typ::Type{Ptr{Float32}}, x::CuArray{Float32, 1, CUDA.DeviceMemory})\n",
      "   @ CUDA ~/.julia/packages/CUDA/2kjXI/src/array.jl:432\n",
      " [4] top-level scope\n",
      "   @ ~/github/Learning/Courses/AdvancedCUDA/part1/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X64sdnNjb2RlLXJlbW90ZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "ccall(:whatever, Nothing, (Ptr{Float32},), CUDA.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, either you need to use different (supported) array operations, or fix the implementation in CUDA.jl. Such a fix can mean using functions from a CUDA library, using existing operations, or writing your own kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, if you need to support this, you can also consider using unified memory. More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Matrix RMSE\n",
    "\n",
    "As a simple exercise, try to implement a function that computes the RMSE of two matrices on the GPU using array operations:\n",
    "\n",
    "$$\n",
    "    RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "Benchmark the implementation against the CPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a CPU implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4087314711248542"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmse(A, B) = sqrt(sum((A-B).^2) / length(A))\n",
    "\n",
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "rmse(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To 'port' this to the GPU, just change the type of the input arrays to `CuArray` and the computation of C just works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4087314711248542"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dA = CuArray(A)\n",
    "dB = CuArray(B)\n",
    "rmse(dA, dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look identical, so let's try benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 1154 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m1.292 ms\u001b[22m\u001b[39m … \u001b[35m9.786 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m 0.00% … 29.43%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m4.211 ms             \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m14.75%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m4.314 ms\u001b[22m\u001b[39m ± \u001b[32m2.609 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m17.35% ± 23.48%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▇\u001b[34m▁\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m█\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m█\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m \u001b[39m█\n",
       "  1.29 ms\u001b[90m     \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m     7.56 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m16.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m6\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "@benchmark rmse($A, $B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m174.982 μs\u001b[22m\u001b[39m … \u001b[35m  5.873 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 40.31%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m179.148 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m186.030 μs\u001b[22m\u001b[39m ± \u001b[32m186.831 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m1.38% ±  1.33%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[34m▇\u001b[39m\u001b[39m▇\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▂\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▃\u001b[39m \u001b[39m▅\n",
       "  175 μs\u001b[90m           Histogram: frequency by time\u001b[39m          183 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m3.47 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m154\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark rmse($dA, $dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive speed-up! Of course, note that we're only measuring the actual computation, and not the time to transfer the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we're simply using `BenchmarkTools.@benchmark` here, measuring execution time of code running on the GPU is generally a little tricky. So let's delve into how to do so in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
